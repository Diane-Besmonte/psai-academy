{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-ByHzumt4umM1VKjHiOQtdlyf3uHpW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='**LangChain** and **LlamaIndex (formerly known as GPT Index)** are both popular frameworks designed to facilitate the development of AI applications involving large language models (LLMs), but they serve different purposes and have distinct features. Here\\'s a comparison to help clarify their differences:\\n\\n### **Purpose and Focus**\\n\\n- **LangChain:**\\n  - Primarily focuses on building **\"chains\"** of multiple language model operations, enabling complex, multi-step workflows.\\n  - Provides tools for task orchestration, prompt management, memory, and integrations with various data sources.\\n  - Designed to create chatbots, question-answering systems, and more sophisticated LLM-powered applications.\\n\\n- **LlamaIndex (GPT Index):**\\n  - Focused on **indexing and querying** large amounts of external data, such as documents, PDFs, or databases.\\n  - Enables efficient retrieval and question-answering over custom datasets by creating embeddings and indices.\\n  - Simplifies the process of ingesting, organizing, and querying data with LLMs.\\n\\n---\\n\\n### **Core Capabilities**\\n\\n| Feature | LangChain | LlamaIndex (GPT Index) |\\n|---|---|---|\\n| **Workflow Composition** | Yes — supports chaining multiple prompts and LLM calls | No — primarily focused on data indexing and retrieval |\\n| **Data Integration** | Supports integration with APIs, databases, document stores | Designed for building indices from external documents and data sources |\\n| **Memory & State Management** | Yes — can maintain conversation state, memory, and context | Limited — mainly maintains context within indices during retrieval |\\n| **Indexing & Retrieval** | Limited — can be used with external retrievers but not its core focus | Yes — built specifically for indexing documents and enabling retrieval-augmented generation (RAG) |\\n| **Prompt Engineering & Management** | Yes — rich prompt templates, tools, and memory | No — mainly focuses on data ingestion and search, less on prompt management |\\n| **Use Cases** | Chatbots, task automation, multi-step reasoning, agent-type applications | Document QA, knowledge base retrieval, building custom search engines |\\n\\n---\\n\\n### **Ecosystem & Integration**\\n\\n- **LangChain:**\\n  - Supports multiple LLM providers (OpenAI, Cohere, AI21, etc.).\\n  - Offers integrations with vector databases, document loaders, and tools.\\n  - Emphasizes extensibility to create complex applications.\\n\\n- **LlamaIndex:**\\n  - Designed for seamless integration with various document sources.\\n  - Can work with vector databases for retrieval.\\n  - Leverages embeddings for indexing.\\n\\n---\\n\\n### **Summary**\\n\\n| Aspect | **LangChain** | **LlamaIndex** |\\n|---|---|---|\\n| **Main Goal** | Orchestrate multi-step LLM workflows and applications | Efficiently index and retrieve information from external datasets |\\n| **Use Cases** | Chatbots, agents, workflows involving multiple prompts | Question answering over large document collections, knowledge bases |\\n| **Strength** | Workflow flexibility, chain building | Data ingestion, fast retrieval, knowledge integration |\\n\\n---\\n\\n### **In Short**\\n- If you want to **build complex conversational agents, chains, or multi-step workflows**, **LangChain** is a strong choice.\\n- If you\\'re focused on **indexing large corpora of documents for quick retrieval and question answering**, **LlamaIndex** provides specialized tools for that purpose.\\n\\n---\\n\\n**Note:** Both frameworks are evolving and sometimes used together for end-to-end solutions—using LlamaIndex to handle data retrieval and LangChain for orchestrating the overall process.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753708562, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=724, prompt_tokens=19, total_tokens=743, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both frameworks that facilitate building applications with large language models (LLMs), but they serve different purposes and have distinct features. Here's a comparative overview:\n",
              "\n",
              "### 1. **Primary Purpose**\n",
              "- **LangChain:**  \n",
              "  Designed as a comprehensive framework to build, orchestrate, and manage complex language model applications. It emphasizes chaining together multiple prompts, models, tools, and data sources to create sophisticated conversational and task-oriented systems.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**  \n",
              "  Focused on indexing, retrieving, and managing large collections of external data (like documents or knowledge bases) to efficiently answer queries using LLMs. It acts as a middleware to connect raw data with LLMs for question-answering and knowledge management.\n",
              "\n",
              "### 2. **Core Functionality**\n",
              "- **LangChain:**  \n",
              "  - Supports prompt templating, chain orchestration, agents, memory management, and integrations with various LLM providers.\n",
              "  - Enables building conversational agents, chatbots, and multi-step workflows.\n",
              "  - Provides tools for connecting to APIs, databases, and other data sources as part of chain pipelines.\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  - Specializes in creating indexes over external data sources (e.g., PDFs, documents, APIs).\n",
              "  - Provides retrieval-augmented generation (RAG) capabilities, where relevant information is retrieved from the index to inform LLM outputs.\n",
              "  - Facilitates building knowledge bases and document-assisted question-answering systems.\n",
              "\n",
              "### 3. **Use Cases**\n",
              "- **LangChain:**  \n",
              "  - Conversational assistants\n",
              "  - Multi-modal workflows\n",
              "  - Automated reasoning tasks\n",
              "  - Tool and plugin integrations\n",
              "  \n",
              "- **LlamaIndex:**  \n",
              "  - Building searchable knowledge bases\n",
              "  - Document retrieval and summarization\n",
              "  - Enhancing factual grounding in LLM responses\n",
              "  - Customized question-answering over large datasets\n",
              "\n",
              "### 4. **Design Philosophy**\n",
              "- **LangChain:**  \n",
              "  Emphasizes modularity and flexibility in constructing complex language model applications with multiple steps and components.\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  Emphasizes efficient data indexing and retrieval to improve LLM performance on domain-specific or large-scale external data.\n",
              "\n",
              "### 5. **Community and Ecosystem**\n",
              "- **LangChain:**  \n",
              "  Has a broad community with many integrations, tutorials, and a focus on building general LLM applications.\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  Have focused on teams building knowledge bases and retrieval-augmented generation systems, often used in combination with LLMs like GPT.\n",
              "\n",
              "---\n",
              "\n",
              "### **Summary Table**\n",
              "\n",
              "| Aspect                  | LangChain                                               | LlamaIndex (GPT Index)                                  |\n",
              "|-------------------------|----------------------------------------------------------|---------------------------------------------------------|\n",
              "| Purpose                 | Full-stack LLM application framework                     | Data indexing and retrieval for LLM augmentation       |\n",
              "| Main Use Cases          | Chatbots, agents, workflows, tool integration             | Search, question-answering, knowledge bases           |\n",
              "| Core Functionality      | Chains, prompts, memory, tool integration                | Indexing, retrieval, RAG                               |\n",
              "| Typical Users           | Developers building complex applications                  | Teams managing large external datasets, knowledge bases |\n",
              "| Flexibility             | Very flexible, supports complex workflows               | Specialized for retrieval and indexing                |\n",
              "\n",
              "---\n",
              "\n",
              "### **In Summary**\n",
              "- **Choose LangChain** if you want to build multi-step applications, conversational agents, or workflows involving LLMs with complex logic and integrations.\n",
              "- **Choose LlamaIndex** if your goal is to create, manage, and query large collections of external documents or data sources to enhance LLM responses with factual grounding.\n",
              "\n",
              "Both can also be used together—LlamaIndex for data retrieval and LangChain for orchestration—depending on your project needs."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you serious? After all this time, you ask me if I prefer crushed ice or cubed ice? Honestly, right now I could scream! Just get me some ice already—preferably whatever's available so I can finally take a breath and stop obsessing over this trivial nonsense."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is fun and adds a refreshing touch to drinks, making them feel more chilled and lively! Cubed ice, on the other hand, is perfect for keeping beverages cold without diluting them too quickly. Both have their charming qualities—depends on the vibe you're going for! 😊❄️"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-ByI0hdrT6PQCFgJNu2VpAJXdexGYA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is fun and adds a refreshing touch to drinks, making them feel more chilled and lively! Cubed ice, on the other hand, is perfect for keeping beverages cold without diluting them too quickly. Both have their charming qualities—depends on the vibe you're going for! 😊❄️\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753708611, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=62, prompt_tokens=30, total_tokens=92, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to significant and lasting changes in global weather patterns, primarily caused by the increase of greenhouse gases such as carbon dioxide and methane in the Earth's atmosphere. Human activities like burning fossil fuels, deforestation, and industrial processes have accelerated these changes. The effects include rising temperatures, melting glaciers and ice caps, more frequent and severe storms, droughts, and shifts in ecosystems. Addressing climate change requires global cooperation to reduce emissions, transition to renewable energy sources, and implement sustainable practices to protect the planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ay nako, mga kababayans, alam niyo na ba? Climate change na talaga ang problema natin ngayon! Parang tula, sumisigaw ang mundo, \"Help, help!\" Pero anong ginagawa natin? Sabi nga nila, \"Action speaks louder than words,\" eh di magkaisa tayo! Mag-reuse, recycle, at magtulungan tayo para sa kinabukasan ng ating planeta. Kasi kung hindi tayo kikilos ngayon, bukas, wala na tayong mabobola pa, nag-iba na ang klima! Kaya laban, laban para sa Mother Earth! Ang dami pa nating pwedeng gawin, mga kababayan—maliit na bagay, malaking epekto. Remember, ang pagbabago ay nagsisimula sa ating maliit na hakbang!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "An example sentence using the word 'crindle' is: During the power outage, I relied on my crindle to stay connected with my team through its glowing signals."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"A 'crindle' is a glowing orb communication device that makes noise to send signals between people through machines. Give a sentence using the word 'crindle'.\"),\n",
        "    assistant_prompt(\"I used my crindle to send a message to my friend across the city — it lit up and made a soft humming sound.\"),\n",
        "    user_prompt(\"A 'crindle' is a glowing orb communication device that sends sound-based signals to other people. Use it in a sentence.\"),\n",
        "    assistant_prompt(\"Whenever my parents wanted to call me downstairs, they would activate the crindle and it would buzz loudly.\"),\n",
        "    user_prompt(\"A 'crindle' is a glowing orb communication device used to send signals. An example sentence using the word 'crindle' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are two \"r\"s in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The word \"strawberry\" has the letters: s, t, r, a, w, b, e, r, r, y. \n",
              "\n",
              "Counting the 'r's: the third letter, the eighth letter, and the ninth letter are 'r's. That's a total of 3.\n",
              "\n",
              "Final number: 3"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "\"In the word 'strawberry', how many times does the letter 'r' appear? Show your reasoning, then only return the final number.\"\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (AI LLM)",
      "language": "python",
      "name": "ai-llm-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
